{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "import base64\n",
    "from google.cloud import vision\n",
    "from google.cloud import storage\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Get combined length of pdfs in directory\n",
    "directory = 'test_files/'\n",
    "total_pages = 0\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    try:\n",
    "        if os.path.isfile(f):\n",
    "            file = open(f'files/{filename}', 'rb')\n",
    "            readpdf = PyPDF2.PdfFileReader(file)\n",
    "            total_pages += readpdf.numPages\n",
    "    except Exception as e:\n",
    "        print(filename)\n",
    "print(total_pages)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"credentials/google/gt-vip-ocr-aca80f32ce4e.json\""
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# https://cloud.google.com/vision/docs/samples/vision-text-detection-pdf-gcs#vision_text_detection_pdf_gcs-python\n",
    "def async_detect_document(gcs_source_uri, gcs_destination_uri):\n",
    "    \"\"\"OCR with PDF/TIFF as source files on GCS\"\"\"\n",
    "    import json\n",
    "    import re\n",
    "    from google.cloud import vision\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # Supported mime_types are: 'application/pdf' and 'image/tiff'\n",
    "    mime_type = 'application/pdf'\n",
    "\n",
    "    # How many pages should be grouped into each json output file.\n",
    "    batch_size = 100\n",
    "\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    feature = vision.Feature(\n",
    "        type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "\n",
    "    gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "    input_config = vision.InputConfig(\n",
    "        gcs_source=gcs_source, mime_type=mime_type)\n",
    "\n",
    "    gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "    output_config = vision.OutputConfig(\n",
    "        gcs_destination=gcs_destination, batch_size=batch_size)\n",
    "\n",
    "    async_request = vision.AsyncAnnotateFileRequest(\n",
    "        features=[feature], input_config=input_config,\n",
    "        output_config=output_config)\n",
    "\n",
    "    operation = client.async_batch_annotate_files(\n",
    "        requests=[async_request])\n",
    "\n",
    "    print('Waiting for the operation to finish.')\n",
    "    operation.result(timeout=420)\n",
    "\n",
    "    # Once the request has completed and the output has been\n",
    "    # written to GCS, we can list all the output files.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)\n",
    "    bucket_name = match.group(1)\n",
    "    prefix = match.group(2)\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List objects with the given prefix, filtering out folders.\n",
    "    blob_list = [blob for blob in list(bucket.list_blobs(\n",
    "        prefix=prefix)) if not blob.name.endswith('/')]\n",
    "    print('Output files:')\n",
    "    for blob in blob_list:\n",
    "        print(blob.name)\n",
    "\n",
    "    # Process the first output file from GCS.\n",
    "    # Since we specified batch_size=2, the first response contains\n",
    "    # the first two pages of the input file.\n",
    "    # output = blob_list[0]\n",
    "\n",
    "    # json_string = output.download_as_string()\n",
    "    # response = json.loads(json_string)\n",
    "\n",
    "    # The actual response for the first page of the input file.\n",
    "    # first_page_response = response['responses'][0]\n",
    "    # annotation = first_page_response['fullTextAnnotation']\n",
    "\n",
    "    # Here we print the full text from the first page.\n",
    "    # The response contains more information:\n",
    "    # annotation/pages/blocks/paragraphs/words/symbols\n",
    "    # including confidence scores and bounding boxes\n",
    "    # print('Full text:\\n')\n",
    "    # print(annotation['text'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# https://towardsdatascience.com/how-to-extract-the-text-from-pdfs-using-python-and-the-google-cloud-vision-api-7a0a798adc13\n",
    "def write_to_text(gcs_destination_uri):\n",
    "    # Once the request has completed and the output has been\n",
    "    # written to GCS, we can list all the output files.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)\n",
    "    bucket_name = match.group(1)\n",
    "    prefix = match.group(2)\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List objects with the given prefix.\n",
    "    blob_list = list(bucket.list_blobs(prefix=prefix))\n",
    "    print('Output files:')\n",
    "\n",
    "    for blob in blob_list:\n",
    "        print(blob.name)\n",
    "\n",
    "    # Process the first output file from GCS.\n",
    "    # Since we specified batch_size=2, the first response contains\n",
    "    # the first two pages of the input file.\n",
    "\n",
    "    doc_text = ''\n",
    "\n",
    "    for n in  range(len(blob_list)):\n",
    "        output = blob_list[n]\n",
    "\n",
    "        json_string = output.download_as_string()\n",
    "        response = json.loads(json_string)\n",
    "\n",
    "\n",
    "        # The actual response for the first page of the input file.\n",
    "        for m in range(len(response['responses'])):\n",
    "\n",
    "\n",
    "            first_page_response = response['responses'][m]\n",
    "\n",
    "            try:\n",
    "                annotation = first_page_response['fullTextAnnotation']\n",
    "            except(KeyError):\n",
    "                print(\"No annotation for this page.\")\n",
    "\n",
    "            # Here we print the full text from the first page.\n",
    "            # The response contains more information:\n",
    "            # annotation/pages/blocks/paragraphs/words/symbols\n",
    "            # including confidence scores and bounding boxes\n",
    "            # print('Full text:\\n')\n",
    "            # print(annotation['text'])\n",
    "            doc_text = doc_text + annotation['text']\n",
    "\n",
    "    return doc_text\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# https://stackoverflow.com/questions/68740510/python-read-all-files-as-gcs-uri-in-google-cloud-storage\n",
    "def get_gcs_bucket_contents(bucket_name, directory, extension):\n",
    "    bucket_list = []\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blobs = client.list_blobs(bucket_name)\n",
    "\n",
    "    for blob in blobs:\n",
    "        if directory in blob.name and extension in blob.name:\n",
    "            bucket_list.append(blob.name)\n",
    "    return bucket_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "bucket_list = get_gcs_bucket_contents('ivanallenarchive', 'input_dir', '.pdf')\n",
    "bucket_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "    ocr_compare_df = pd.read_parquet('ocr_compare_df.parquet.gzip', engine='pyarrow')\n",
    "except Exception as e:\n",
    "    df_dict = {'item_filename'      : [], 'Google_Vision': []}\n",
    "    ocr_compare_df = pd.DataFrame(df_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for i, item in enumerate(bucket_list):\n",
    "    if i < 3:\n",
    "        pass\n",
    "    output_file_name =  re.sub('input_dir/', '', item)\n",
    "    output_file_name =  re.sub('.pdf', '', output_file_name)\n",
    "\n",
    "    gsurl_s = f'gs://ivanallenarchive/{item}'\n",
    "    gsurl_d = f'gs://ivanallenarchive/output_dir/{output_file_name}'\n",
    "    async_detect_document(gsurl_s, gsurl_d)\n",
    "    doc_text = write_to_text(gsurl_d)\n",
    "    new_text_dict = {'item_filename'    :  output_file_name , 'Google_Vision': doc_text}\n",
    "    ocr_compare_df = pd.concat([ocr_compare_df, pd.DataFrame.from_records([new_text_dict])])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ocr_compare_df.to_parquet('raw_data_df.parquet.gzip', compression='gzip')"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ocr_compare_df.to_excel('test.xlsx')"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "item = 'input_dir/0a57f7837a091ce523ddbca495a38198.pdf'\n",
    "output_file_name =  re.sub('input_dir/', '', item)\n",
    "output_file_name =  re.sub('.pdf', '', output_file_name)\n",
    "\n",
    "gsurl_s = f'gs://ivanallenarchive/{item}'\n",
    "gsurl_d = f'gs://ivanallenarchive/output_dir/{output_file_name}'\n",
    "async_detect_document(gsurl_s, gsurl_d)\n",
    "doc_text = write_to_text(gsurl_d)\n",
    "new_text_dict = {'item_filename'    :  output_file_name , 'Google_Vision': doc_text}\n",
    "ocr_compare_df = pd.concat([ocr_compare_df, pd.DataFrame.from_records([new_text_dict])])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "gsurl_d = f'gs://ivanallenarchive/output_dir/0a57f7837a091ce523ddbca495a38198output-1-to-6'\n",
    "\n",
    "doc_text = write_to_text(gsurl_d)\n",
    "new_text_dict = {'item_filename'    :  output_file_name , 'Google_Vision': doc_text}\n",
    "ocr_compare_df = pd.concat([ocr_compare_df, pd.DataFrame.from_records([new_text_dict])])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ocr_compare_df.to_excel('test.xlsx')"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
